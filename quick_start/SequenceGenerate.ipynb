{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take BA.2.1 as parent node to conduct PLM finetuning and sequence generating. We provide a sampled initial sequence set with 1000 sequences for a quick demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "rbd_name = 'BA.2.1'\n",
    "\n",
    "save_folder = \"ckpt/finetune\"\n",
    "batch_size = 16\n",
    "\n",
    "init_data_path = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# model finetuning\n",
    "def get_training_sequences(path):\n",
    "    seqs = []\n",
    "    f = open(path, 'r')\n",
    "    for line in f:\n",
    "        seqs.append(line.strip())\n",
    "\n",
    "    return seqs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "train_sequences = get_training_sequences(init_data_path)\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "train_dataset = Dataset.from_dict(train_tokenized)\n",
    "train_args = TrainingArguments(\n",
    "        output_dir=save_folder,\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=5,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=1000,\n",
    "        report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "\n",
    "from collator import DataCollatorForMaskedGeneration\n",
    "\n",
    "rbd_dict={\n",
    "    'BA.2.1':   'NITNLCPFDEVFNATRFASVYAWNRKRISNCVADYSVLYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGNKPCNGVAGFNCYFPLRSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "    'BA.5.1':   'NITNLCPFDEVFNATRFASVYAWNRKRISNCVADYSVLYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKVGGNYNYRYRLFRKSNLKPFERDISTEIYQAGNKPCNGVAGVNCYFPLQSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "    'XBB.1.5':  'NITNLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKPSGNYNYLYRLFRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "}\n",
    "mutation_dict={\n",
    "    'BA.2.1':   ['G339D','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','S477N','T478K','E484A','Q493R','Q498R','N501Y','Y505H'],\n",
    "    'BA.5.1':   ['G339D','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','L452R','S477N','T478K','E484A','F486V','Q498R','N501Y','Y505H'],\n",
    "    'XBB.1.5':  ['G339H','R346T','L368I','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','V445P','G446S','N460K','S477N','T478K','E484A','F486P','F490S','Q498R','N501Y','Y505H'],\n",
    "}\n",
    "\n",
    "def get_index(line):\n",
    "    return int(re.findall(r'[A-Z]([0-9]+)[A-Z]',line)[0])\n",
    "def get_mut(line):\n",
    "    return re.findall(r'[A-Z][0-9]+([A-Z])',line)[0]\n",
    "\n",
    "def numpy_mask_tokens(inputs, probility_mutation, mask_token_id):\n",
    "    \"\"\"\n",
    "    Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "    \"\"\"\n",
    "    # Numpy doesn't have bernoulli, so we use a binomial with 1 trial\n",
    "    masked_indices = np.random.binomial(1, probility_mutation, size=probility_mutation.shape).astype(bool)\n",
    "    masked_lm_positions = np.where(masked_indices == True)[0]\n",
    "    # The rest of the time (10% of the time) we keep the masked input tokens unchanged\n",
    "    inputs[masked_lm_positions] = mask_token_id\n",
    "    return inputs, masked_lm_positions\n",
    "\n",
    "\n",
    "class GenerateDataset(Dataset):\n",
    "    def __init__(self, num_samples, rbd_seq, tokenizer):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.rbd_seq = rbd_seq\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenizer(self.rbd_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def dump(seqs, name, path):\n",
    "    file_name = os.path.join(path, \"mutation_{}_{}.txt\".format(rbd_name, name))\n",
    "    print('seq num: {}, save path: {}'.format(len(seqs), file_name))\n",
    "    f = open(file_name, \"w\")\n",
    "    for seq in seqs:\n",
    "        f.write(seq + \"\\n\")\n",
    "    f.close()\n",
    "\n",
    "\n",
    "def _check(path):\n",
    "    raw = rbd_dict['BA.2.1']\n",
    "    nums = np.zeros(len(raw))\n",
    "    seqs = set()\n",
    "    for i in range(10):\n",
    "        path_ = os.path.join(path, 'mutation_BA.2.1_{}.txt'.format(i))\n",
    "        f = open(path_, 'r')\n",
    "        for line in f:\n",
    "            nums += (np.array(list(line.strip())) != np.array(list(raw)))\n",
    "            seqs.add(line.strip())\n",
    "    print(nums)\n",
    "    print(len(seqs))\n",
    "    plt.plot(nums)\n",
    "    plt.savefig('plot.png')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    rbd_name = 'BA.2.1' # 'BA.5.1' # 'XBB.1.5'\n",
    "    ft_path = 'ckpt/finetune/checkpoint-xx'\n",
    "    data_root = 'data/'\n",
    "    site_freq_path = 'ckpt/site_mutation_frequency/{}_mutation_frequency_203.npy'.format(rbd_name)\n",
    "    mutation_save_path = os.path.join(data_root, 'raw_seqs')\n",
    "\n",
    "    total_number = 1_000_000\n",
    "    step = 10_000\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(ft_path)\n",
    "    model = AutoModelForMaskedLM.from_pretrained(ft_path).to(device).eval()\n",
    "\n",
    "    max_len = 203\n",
    "    max_mask = 5\n",
    "    topk = 10\n",
    "    batch_size = 4\n",
    "\n",
    "    rbd_seq = rbd_dict[rbd_name]\n",
    "    rbd_id = np.array(tokenizer(rbd_seq)['input_ids'])\n",
    "\n",
    "    save_steps=np.arange(0, total_number+1, step)[1:]\n",
    "    os.makedirs(mutation_save_path, exist_ok=True)\n",
    "\n",
    "    probility_mutation = np.load(site_freq_path)\n",
    "\n",
    "    collator = DataCollatorForMaskedGeneration(tokenizer, torch.tensor(probility_mutation), max_mask, device=device)\n",
    "\n",
    "    dataset = GenerateDataset(num_samples=total_number*100, \n",
    "                              rbd_seq=rbd_seq, \n",
    "                              tokenizer=tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output_seq = set()\n",
    "        output_seq_tp = set()\n",
    "        process = 0\n",
    "        for i, data in enumerate(dataloader):\n",
    "            if i % 500 == 0:\n",
    "                print('>>>>> {} loops, current sequence num: {}'.format(i, len(output_seq)))\n",
    "            token_ids_ = data['token_ids'].cpu()\n",
    "            data.pop('token_ids')\n",
    "\n",
    "            out = model(**data)\n",
    "            indices = torch.topk(out['logits'], topk, dim = -1).indices.cpu()\n",
    "            bs = indices.shape[0]\n",
    "            indices = indices.reshape(-1, topk)\n",
    "            for _ in range(20):\n",
    "                token_ids = token_ids_.clone()\n",
    "                index_ran = np.random.randint(0, topk, size=(indices.shape[0]))\n",
    "                predict_id = indices[range(indices.shape[0]), index_ran]\n",
    "                predict_id = predict_id.reshape(bs, -1)\n",
    "\n",
    "                mask_ = (token_ids == tokenizer.mask_token_id)&(predict_id >= 4)&(predict_id <= 23)\n",
    "                token_ids[mask_] = predict_id[mask_]\n",
    "\n",
    "                sequences = tokenizer.batch_decode(token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "                for sequence in sequences:\n",
    "                    sequence = re.sub(r'\\s', '', sequence)\n",
    "                    if sequence not in output_seq:\n",
    "                        output_seq.add(sequence)\n",
    "                        output_seq_tp.add(sequence)\n",
    "\n",
    "                    if process >= len(save_steps):\n",
    "                        break\n",
    "                    if len(output_seq) >= save_steps[process]:\n",
    "                        print('process {}, output_seq: {}, output_seq_tp: {}'.format(process, len(output_seq), len(output_seq_tp)))\n",
    "                        dump(output_seq_tp, process, mutation_save_path)\n",
    "                        process+=1\n",
    "                        output_seq_tp=set()\n",
    "\n",
    "                if process >= len(save_steps):\n",
    "                    break\n",
    "            if process >= len(save_steps):\n",
    "                break\n",
    "        print(total_number, len(output_seq))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
