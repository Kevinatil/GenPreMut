{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence Generate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We take BA.2.1 as parent node to conduct PLM finetuning and sequence generating. We provide a sampled initial sequence set with 10000 sequences for a quick demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/Kevinatil/GenPreMut.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
    "from datasets import Dataset as Dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collator from sequence_generate/sequence_generate/collator.py\n",
    "from dataclasses import dataclass\n",
    "from typing import Any, Callable, Dict, List, NewType, Optional, Tuple, Union\n",
    "from collections.abc import Mapping\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from transformers.data.data_collator import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DataCollatorForMaskedGeneration:\n",
    "\n",
    "    tokenizer: PreTrainedTokenizerBase\n",
    "    mlm_probability: Any = 0.15\n",
    "    max_mask: int = 5\n",
    "    device: Any = 'cuda'\n",
    "    pad_to_multiple_of: Optional[int] = None\n",
    "    tf_experimental_compile: bool = False\n",
    "    return_tensors: str = \"pt\"\n",
    "\n",
    "\n",
    "    def __call__(self, features):\n",
    "        return self.torch_call(features)\n",
    "\n",
    "    def torch_call(self, examples: List[Union[List[int], Any, Dict[str, Any]]]) -> Dict[str, Any]:\n",
    "        batch = self.tokenizer.pad(examples, return_tensors=\"pt\", pad_to_multiple_of=self.pad_to_multiple_of)\n",
    "\n",
    "        special_tokens_mask = batch.pop(\"special_tokens_mask\", None)\n",
    "        input_ids, len_ = self.torch_mask_tokens(\n",
    "            batch[\"input_ids\"], special_tokens_mask=special_tokens_mask\n",
    "        )\n",
    "        batch[\"token_ids\"] = batch[\"input_ids\"] # no mask\n",
    "        batch[\"input_ids\"] = input_ids # has mask\n",
    "        for key, value in batch.items():\n",
    "            batch[key] = value[:len_].to(self.device)\n",
    "        return batch\n",
    "\n",
    "    def torch_mask_tokens(self, inputs: Any, special_tokens_mask: Optional[Any] = None) -> Tuple[Any, Any]:\n",
    "        \"\"\"\n",
    "        Prepare masked tokens inputs/labels for masked language modeling: 80% MASK, 10% random, 10% original.\n",
    "        \"\"\"\n",
    "        while True:\n",
    "            if type(self.mlm_probability) == float:\n",
    "                # We sample a few tokens in each sequence for MLM training (with probability `self.mlm_probability`)\n",
    "                probability_matrix = torch.full(inputs.shape, self.mlm_probability)\n",
    "            else:\n",
    "                probability_matrix = self.mlm_probability.repeat(inputs.shape[0], 1)\n",
    "\n",
    "            if special_tokens_mask is None:\n",
    "                special_tokens_mask = [\n",
    "                    self.tokenizer.get_special_tokens_mask(val, already_has_special_tokens=True) for val in inputs.tolist()\n",
    "                ]\n",
    "                special_tokens_mask = torch.tensor(special_tokens_mask, dtype=torch.bool)\n",
    "            else:\n",
    "                special_tokens_mask = special_tokens_mask.bool()\n",
    "\n",
    "            probability_matrix.masked_fill_(special_tokens_mask, value=0.0)\n",
    "            masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "            # labels[~masked_indices] = -100  # We only compute loss on masked tokens\n",
    "\n",
    "            inputs[masked_indices] = self.tokenizer.convert_tokens_to_ids(self.tokenizer.mask_token)\n",
    "\n",
    "            masked_indices = (masked_indices.sum(dim=1) < self.max_mask) & (masked_indices.sum(dim=1) >= 1)\n",
    "\n",
    "            if masked_indices.any():\n",
    "                return inputs[masked_indices], masked_indices.sum().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbd_name = 'BA.2.1'\n",
    "\n",
    "\n",
    "data_root = 'GenPreMut/data'\n",
    "model_root = 'GenPreMut/ckpt'\n",
    "\n",
    "save_folder = os.path.join(model_root, \"finetune\")\n",
    "batch_size = 2\n",
    "\n",
    "init_data_path = os.path.join(data_root, \"finetune/sample_{}.txt\".format(rbd_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model finetuning\n",
    "def get_training_sequences(path):\n",
    "    seqs = []\n",
    "    f = open(path, 'r')\n",
    "    for line in f:\n",
    "        seqs.append(line.strip())\n",
    "\n",
    "    return seqs\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"facebook/esm2_t33_650M_UR50D\")\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n",
    "\n",
    "\n",
    "train_sequences = get_training_sequences(init_data_path)\n",
    "train_tokenized = tokenizer(train_sequences)\n",
    "train_dataset = Dataset_.from_dict(train_tokenized)\n",
    "train_args = TrainingArguments(\n",
    "        output_dir=save_folder,\n",
    "        save_strategy = \"epoch\",\n",
    "        learning_rate=1e-4,\n",
    "        per_device_train_batch_size=batch_size,\n",
    "        per_device_eval_batch_size=batch_size,\n",
    "        num_train_epochs=1,\n",
    "        weight_decay=0.01,\n",
    "        warmup_steps=1000,\n",
    "        report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    train_args,\n",
    "    train_dataset=train_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rbd_dict={\n",
    "    'BA.2.1':   'NITNLCPFDEVFNATRFASVYAWNRKRISNCVADYSVLYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKVGGNYNYLYRLFRKSNLKPFERDISTEIYQAGNKPCNGVAGFNCYFPLRSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "    'BA.5.1':   'NITNLCPFDEVFNATRFASVYAWNRKRISNCVADYSVLYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKVGGNYNYRYRLFRKSNLKPFERDISTEIYQAGNKPCNGVAGVNCYFPLQSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "    'XBB.1.5':  'NITNLCPFHEVFNATTFASVYAWNRKRISNCVADYSVIYNFAPFFAFKCYGVSPTKLNDLCFTNVYADSFVIRGNEVSQIAPGQTGNIADYNYKLPDDFTGCVIAWNSNKLDSKPSGNYNYLYRLFRKSKLKPFERDISTEIYQAGNKPCNGVAGPNCYSPLQSYGFRPTYGVGHQPYRVVVLSFELLHAPATVCGPKKST',\n",
    "}\n",
    "mutation_dict={\n",
    "    'BA.2.1':   ['G339D','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','S477N','T478K','E484A','Q493R','Q498R','N501Y','Y505H'],\n",
    "    'BA.5.1':   ['G339D','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','L452R','S477N','T478K','E484A','F486V','Q498R','N501Y','Y505H'],\n",
    "    'XBB.1.5':  ['G339H','R346T','L368I','S371F','S373P','S375F','T376A','D405N','R408S','K417N','N440K','V445P','G446S','N460K','S477N','T478K','E484A','F486P','F490S','Q498R','N501Y','Y505H'],\n",
    "}\n",
    "\n",
    "def get_index(line):\n",
    "    return int(re.findall(r'[A-Z]([0-9]+)[A-Z]',line)[0])\n",
    "def get_mut(line):\n",
    "    return re.findall(r'[A-Z][0-9]+([A-Z])',line)[0]\n",
    "\n",
    "def numpy_mask_tokens(inputs, probility_mutation, mask_token_id):\n",
    "\n",
    "    masked_indices = np.random.binomial(1, probility_mutation, size=probility_mutation.shape).astype(bool)\n",
    "    masked_lm_positions = np.where(masked_indices == True)[0]\n",
    "    inputs[masked_lm_positions] = mask_token_id\n",
    "    return inputs, masked_lm_positions\n",
    "\n",
    "\n",
    "class GenerateDataset(Dataset):\n",
    "    def __init__(self, num_samples, rbd_seq, tokenizer):\n",
    "        super().__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.rbd_seq = rbd_seq\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.tokenizer(self.rbd_seq)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "def dump(seqs, name, path):\n",
    "    file_name = os.path.join(path, \"mutation_{}_{}.txt\".format(rbd_name, name))\n",
    "    print('seq num: {}, save path: {}'.format(len(seqs), file_name))\n",
    "    f = open(file_name, \"w\")\n",
    "    for seq in seqs:\n",
    "        f.write(seq + \"\\n\")\n",
    "    f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_ckpt(path):\n",
    "    ckpts =os.listdir(path)\n",
    "    max_ = 0\n",
    "    for ckpt in ckpts:\n",
    "        find = re.findall(r'checkpoint-([0-9]+)', ckpt)\n",
    "        if len(find):\n",
    "            max_ = max(max_, int(find[0]))\n",
    "    return max_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence generate\n",
    "device = torch.device(\"cuda\")\n",
    "ft_path = os.path.join(model_root, 'finetune/checkpoint-{}'.format(get_latest_ckpt(os.path.join(model_root, 'finetune'))))\n",
    "site_freq_path = os.path.join(model_root, 'site_mutation_frequency/{}_mutation_frequency_203.npy'.format(rbd_name))\n",
    "mutation_save_path = os.path.join(data_root, 'raw_seqs')\n",
    "\n",
    "total_number = 1000\n",
    "step = 100\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(ft_path)\n",
    "model = AutoModelForMaskedLM.from_pretrained(ft_path).to(device).eval()\n",
    "\n",
    "max_len = 203\n",
    "max_mask = 5\n",
    "topk = 10\n",
    "batch_size = 4\n",
    "\n",
    "rbd_seq = rbd_dict[rbd_name]\n",
    "rbd_id = np.array(tokenizer(rbd_seq)['input_ids'])\n",
    "\n",
    "save_steps=np.arange(0, total_number+1, step)[1:]\n",
    "os.makedirs(mutation_save_path, exist_ok=True)\n",
    "\n",
    "probility_mutation = np.load(site_freq_path)\n",
    "collator = DataCollatorForMaskedGeneration(tokenizer, torch.tensor(probility_mutation), max_mask, device=device)\n",
    "\n",
    "dataset = GenerateDataset(num_samples=total_number*100, \n",
    "                          rbd_seq=rbd_seq, \n",
    "                          tokenizer=tokenizer)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, collate_fn=collator)\n",
    "\n",
    "with torch.no_grad():\n",
    "    output_seq = set()\n",
    "    output_seq_tp = set()\n",
    "    process = 0\n",
    "    for i, data in enumerate(dataloader):\n",
    "        if i % 500 == 0:\n",
    "            print('>>>>> {} loops, current sequence num: {}'.format(i, len(output_seq)))\n",
    "        token_ids_ = data['token_ids'].cpu()\n",
    "        data.pop('token_ids')\n",
    "\n",
    "        out = model(**data)\n",
    "        indices = torch.topk(out['logits'], topk, dim = -1).indices.cpu()\n",
    "        bs = indices.shape[0]\n",
    "        indices = indices.reshape(-1, topk)\n",
    "        for _ in range(20):\n",
    "            token_ids = token_ids_.clone()\n",
    "            index_ran = np.random.randint(0, topk, size=(indices.shape[0]))\n",
    "            predict_id = indices[range(indices.shape[0]), index_ran]\n",
    "            predict_id = predict_id.reshape(bs, -1)\n",
    "\n",
    "            mask_ = (token_ids == tokenizer.mask_token_id)&(predict_id >= 4)&(predict_id <= 23)\n",
    "            token_ids[mask_] = predict_id[mask_]\n",
    "\n",
    "            sequences = tokenizer.batch_decode(token_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "\n",
    "            for sequence in sequences:\n",
    "                sequence = re.sub(r'\\s', '', sequence)\n",
    "                if sequence not in output_seq:\n",
    "                    output_seq.add(sequence)\n",
    "                    output_seq_tp.add(sequence)\n",
    "\n",
    "                if process >= len(save_steps):\n",
    "                    break\n",
    "                if len(output_seq) >= save_steps[process]:\n",
    "                    print('process {}, output_seq: {}, output_seq_tp: {}'.format(process, len(output_seq), len(output_seq_tp)))\n",
    "                    dump(output_seq_tp, process, mutation_save_path)\n",
    "                    process+=1\n",
    "                    output_seq_tp=set()\n",
    "\n",
    "            if process >= len(save_steps):\n",
    "                break\n",
    "        if process >= len(save_steps):\n",
    "            break\n",
    "    print(total_number, len(output_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check(path, num):\n",
    "    raw = rbd_dict['BA.2.1']\n",
    "    nums = np.zeros(len(raw))\n",
    "    seqs = set()\n",
    "    for i in range(num):\n",
    "        path_ = os.path.join(path, 'mutation_BA.2.1_{}.txt'.format(i))\n",
    "        f = open(path_, 'r')\n",
    "        for line in f:\n",
    "            nums += (np.array(list(line.strip())) != np.array(list(raw)))\n",
    "            seqs.add(line.strip())\n",
    "    plt.title('site mutation frequency of generated sequences')\n",
    "    plt.plot(nums)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# site mutation frequency\n",
    "%matplotlib inline\n",
    "plt.title('site mutation frequency of initial sequences')\n",
    "plt.plot(np.load(site_freq_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_check(mutation_save_path, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mamba",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
